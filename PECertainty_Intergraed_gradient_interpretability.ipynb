{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "# load train_df and test_df\n",
    "train_dataset = Dataset(pa.Table.from_pandas(train_df))\n",
    "test_dataset = Dataset(pa.Table.from_pandas(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('used_all_setfit_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PH0NK0l9Exx"
   },
   "source": [
    "# Code below is for running integrated gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TPp-PWy08c8w"
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from datasets import load_dataset,Dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from setfit_ig.html_text_colorizer import WordImportanceColorsSetFit\n",
    "from setfit_ig.integrated_gradients import integrated_gradients_on_text\n",
    "from setfit_ig.model_head import SklearnToPyTorchLogisticRegression\n",
    "\n",
    "from setfit_ig.setfit_extensions import SetFitGrad, SetFitModelWithTorchHead\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJSlbia3Hgsr"
   },
   "source": [
    "## SetFitGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZoJ2I-osP-nV"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import torch\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "from setfit import SetFitModel\n",
    "from torch.autograd import grad\n",
    "from torch.nn import Sequential\n",
    "\n",
    "\n",
    "\n",
    "class SklearnToPyTorchLogisticRegression(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pass a trained sklearn LogisticRegression model to this class\n",
    "    to create an equivalent PyTorch model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sklearn_model):\n",
    "        super(SklearnToPyTorchLogisticRegression, self).__init__()\n",
    "\n",
    "        # Extract the parameters from the sklearn model\n",
    "        coef = sklearn_model.coef_.T\n",
    "        intercept = sklearn_model.intercept_\n",
    "\n",
    "        # Initialize the PyTorch parameters\n",
    "        self.linear = torch.nn.Linear(coef.shape[1], 3)\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.copy_(torch.from_numpy(coef).cuda())\n",
    "            self.linear.bias.copy_(torch.from_numpy(intercept).cuda())\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return torch.softmax(out)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.forward(x).round().squeeze().int()\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Compute the predicted probabilities of the positive class for input x\n",
    "        return self.forward(x).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "class SetFitGrad:\n",
    "    \"\"\"\n",
    "    This class takes a SetFit model and deconstructs its operations to\n",
    "    allow for exploration of gradients.\n",
    "\n",
    "    Essentially, instead of passing a sentence and getting the probability of a class,\n",
    "    we can pass a token embedding tensor and do the same + return the gradient w.r.t to\n",
    "    each token embedding dimension.\n",
    "\n",
    "    NOTE: This assumes we are interested in a binary classification problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: SetFitModel, tokenizer=None, device: int = None):\n",
    "        self.model_body = model.model_body\n",
    "        self.model_head = model.model_head\n",
    "\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = self.model_body.device\n",
    "\n",
    "        if tokenizer:\n",
    "            self.tokenizer = tokenizer\n",
    "        else:\n",
    "            self.tokenizer = lambda x: self.model_body.tokenizer(\n",
    "                x, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "        transformer = self.model_body._modules[\"0\"]._modules[\"auto_model\"]\n",
    "        self.embedder = transformer._modules[\"embeddings\"]\n",
    "        self.encoder = transformer._modules[\"encoder\"]\n",
    "\n",
    "        rest_of_processing = OrderedDict(\n",
    "            {\n",
    "                key: value\n",
    "                for key, value in self.model_body._modules.items()\n",
    "                if key != \"0\"\n",
    "            }\n",
    "        )\n",
    "        self.rest_of_processing = Sequential(rest_of_processing)\n",
    "\n",
    "    def model_pass(self, sentence_string: str = None, sentence_token_embedding=None):\n",
    "\n",
    "        if sentence_token_embedding is None:\n",
    "            with torch.no_grad():\n",
    "                sentence = self.tokenizer(sentence_string).to(device=self.device)\n",
    "                sentence_token_embedding = self.embedder(\n",
    "                    input_ids=sentence[\"input_ids\"],\n",
    "                    # token_type_ids=sentence[\"token_type_ids\"],\n",
    "                )\n",
    "\n",
    "            attention_mask = sentence[\"attention_mask\"]\n",
    "\n",
    "            sentence_token_embedding.requires_grad = True\n",
    "            input_ids = sentence[\"input_ids\"]\n",
    "        else:\n",
    "\n",
    "            input_ids = None\n",
    "            attention_mask_dim = sentence_token_embedding.shape[1:2]\n",
    "            attention_mask = torch.ones(attention_mask_dim, device=self.device)\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            sentence_token_embedding, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        features = {}\n",
    "        features[\"token_embeddings\"] = encoder_output[0]\n",
    "        features[\"attention_mask\"] = attention_mask\n",
    "        features[\"input_ids\"] = input_ids\n",
    "\n",
    "        results = self.rest_of_processing(features)\n",
    "\n",
    "        positive_class_probability = self.model_head.predict_proba(\n",
    "            results[\"sentence_embedding\"]\n",
    "        )\n",
    "\n",
    "        token_embedding_gradients = grad(\n",
    "            outputs=positive_class_probability,\n",
    "            inputs=sentence_token_embedding,\n",
    "            grad_outputs=torch.ones_like(positive_class_probability),\n",
    "            retain_graph=True,\n",
    "        )[0].squeeze()\n",
    "\n",
    "        output = namedtuple(\n",
    "            \"SetFitGrad\",\n",
    "            [\n",
    "                \"positive_class_probability\",\n",
    "                \"token_embedding_gradients\",\n",
    "                \"sentence_token_embedding\",\n",
    "                \"attention_mask\",\n",
    "                \"input_ids\",\n",
    "                \"sentence_embedding\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return output(\n",
    "            positive_class_probability,\n",
    "            token_embedding_gradients,\n",
    "            sentence_token_embedding,\n",
    "            attention_mask,\n",
    "            input_ids,\n",
    "            results[\"sentence_embedding\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF6Cl9dOHeay"
   },
   "source": [
    "## IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "LUEHnEdDVyLA",
    "outputId": "4a0780d0-8ae7-4627-c3df-ddccddc6dbe5"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.special import roots_legendre\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def integrated_gradients_on_text(\n",
    "    sentence_string: str, grd: SetFitGrad, integration_steps: int = 100\n",
    "):\n",
    "    \"\"\"\n",
    "    Implementation of integrated gradients method for attribution.\n",
    "\n",
    "    Returns a dataframe with each word in \"text\" alongside their importance to the\n",
    "    class label being 1. Essentially a bit more refined than calculating the gradient\n",
    "    of the probability with respect to each word. Positive gradients indicate words that\n",
    "    would increase the probability and negative gradients the opposite.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    device = grd.model_body.device\n",
    "\n",
    "    prob, _, target_embed, _, input_ids, _ = grd.model_pass(\n",
    "        sentence_string=sentence_string\n",
    "    )\n",
    "\n",
    "\n",
    "    # TODO encode an empty sentence instead of passing zeros.\n",
    "    init_embed = torch.zeros_like(target_embed, device=device)\n",
    "\n",
    "    # don't zero out [CLS] and [SEP] tokens\n",
    "    init_embed[0, 0, :] = target_embed[0, 0, :]\n",
    "    init_embed[0, -1, :] = target_embed[0, -1, :]\n",
    "\n",
    "    (\n",
    "        final_scores,\n",
    "        grad_per_integration_step,\n",
    "    ) = calculate_integrated_gradient_scores(\n",
    "        grd, integration_steps, init_embed, target_embed\n",
    "    )\n",
    "\n",
    "    scores = pd.DataFrame(\n",
    "        {\n",
    "            \"token\": grd.model_body.tokenizer.convert_ids_to_tokens(\n",
    "                input_ids.squeeze()\n",
    "            ),\n",
    "            \"token_ids\": input_ids.squeeze().to(\"cpu\"),\n",
    "            \"attribution_score\": final_scores,\n",
    "        }\n",
    "    ).set_index(\"token_ids\")\n",
    "\n",
    "    word_to_ids = construct_word_to_id_mapping(sentence_string, grd)\n",
    "\n",
    "    word_to_score = []\n",
    "    for word, token_ids in word_to_ids:\n",
    "        key_scores = scores.loc[token_ids, \"attribution_score\"].sum()\n",
    "        word_to_score.append((word, key_scores))\n",
    "\n",
    "    df_word_to_score = (\n",
    "        pd.DataFrame(word_to_score).rename(columns={0: \"words\", 1: \"score\"}).dropna()\n",
    "    )\n",
    "\n",
    "    return df_word_to_score, prob, grad_per_integration_step\n",
    "\n",
    "\n",
    "def calculate_integrated_gradient_scores(\n",
    "    grd: SetFitGrad,\n",
    "    num_of_integration_steps: int,\n",
    "    init_embed: torch.Tensor,\n",
    "    target_embed: torch.Tensor,\n",
    "    max_alpha: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    grd: SetFitGrad\n",
    "    num_of_integration_steps: int\n",
    "    init_embed: torch, 1 x number of tokens x embedding size\n",
    "    target_embed: torch.Tensor, 1 x number of tokens x embedding size\n",
    "    max_alpha: float, up to where to estimate the integral of the gradient curve.\n",
    "    \"\"\"\n",
    "    device = grd.model_body.device\n",
    "\n",
    "    integration_steps, weights = roots_legendre(num_of_integration_steps)\n",
    "\n",
    "    # originally the steps are in (-1,1), need to map to (0,1)\n",
    "    integration_steps = numpy.interp(integration_steps, (-1, 1), (0, max_alpha))\n",
    "    integration_steps = torch.tensor(integration_steps, device=device)\n",
    "\n",
    "    # scale the weights to the new interval\n",
    "    weights = torch.tensor(weights, device=\"cpu\") * max_alpha / 2.0\n",
    "\n",
    "    new_embed_v = torch.einsum(\n",
    "        \"bp,bqr->bpqr\", integration_steps[None, :], target_embed - init_embed\n",
    "    ).squeeze()\n",
    "    new_embed_v = new_embed_v + init_embed\n",
    "    new_embed_v = new_embed_v.type(torch.float32)\n",
    "\n",
    "    gradient_at_every_perturbation = grd.model_pass(\n",
    "        sentence_token_embedding=new_embed_v\n",
    "    )[1].cpu()\n",
    "\n",
    "    diff = (target_embed - init_embed).cpu().detach()\n",
    "\n",
    "    weighted_grads_per_integration_step = (\n",
    "        gradient_at_every_perturbation\n",
    "        * diff[:, None, None, :]\n",
    "        * weights[None, :, None, None]\n",
    "    )\n",
    "\n",
    "    integrals_per_embedding = weighted_grads_per_integration_step.squeeze().sum(\n",
    "        axis=0\n",
    "    )  # number of tokens x embedding dim\n",
    "\n",
    "    final_scores = integrals_per_embedding.mean(axis=1)\n",
    "    return final_scores, weighted_grads_per_integration_step\n",
    "\n",
    "\n",
    "def construct_word_to_id_mapping(sentence_string, grd) -> Tuple[str, List[int]]:\n",
    "\n",
    "    tok_sentence = grd.model_body.tokenizer.tokenize(sentence_string)\n",
    "\n",
    "    # temp = grd.model_body.tokenizer.tokenize(sentence_string)\n",
    "    # tok_sentence = []\n",
    "    # for word in temp:\n",
    "    #     tok_sentence.append(word.replace(\"\\u0120\",\"\"))\n",
    "\n",
    "    # print(tok_sentence)\n",
    "\n",
    "    encoding = grd.model_body.tokenizer.encode(\n",
    "        sentence_string, add_special_tokens=False\n",
    "    )\n",
    "    word_to_ids = [(x, y) for x, y in zip(tok_sentence, encoding)]\n",
    "    return word_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kxb3N53cHaXW"
   },
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize, rgb2hex\n",
    "\n",
    "\n",
    "def hlstr(string: str, color=\"white\") -> str:\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<span style=background-color:{color}>{string} </span>\"\n",
    "\n",
    "\n",
    "def colorize(attrs, cmap=\"PiYG\"):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO pass an option to have this absolute or relative colouring\n",
    "\n",
    "    # map colors separately for positive and negative elements\n",
    "    attrs_ = attrs.copy()\n",
    "    pos = attrs_[attrs_ > 0]\n",
    "    # pos_median = np.median(pos)\n",
    "    pos_median =np.percentile(pos, 75)  \n",
    "    pos_p  = attrs_[attrs_ >= pos_median]\n",
    "    pos_s  = attrs_[(attrs_ < pos_median) & (attrs_>0)]\n",
    "\n",
    "    # attrs_[attrs_ > 0] = numpy.interp(pos, (pos.min(), pos.max()), (0.5, 0.9))\n",
    "    attrs_[attrs_ >=pos_median] = numpy.interp(pos_p, (pos_p.min(), pos_p.max()), (0.77, 0.77))\n",
    "    attrs_[(attrs_ < pos_median) & (attrs_>0)] = numpy.interp(pos_s, (pos_s.min(), pos_s.max()), (0.6, 0.6))\n",
    "\n",
    "    neg = attrs_[attrs_ < 0]\n",
    "    # attrs_[attrs_ < 0] = numpy.interp(neg, (neg.min(), neg.max()), (0.25, 0.5))\n",
    "    attrs_[attrs_ <= 0] = numpy.interp(neg, (neg.min(), neg.max()), (0.5, 0.5))\n",
    "\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "    cmap = cm.get_cmap(cmap)\n",
    "\n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: rgb2hex(cmap(norm(x))), attrs_))\n",
    "\n",
    "\n",
    "    # c = list(map(lambda x: tuple(round(255 * j) for j in x[:3]) , attrs))\n",
    "    # return colors\n",
    "    return colors\n",
    "\n",
    "\n",
    "class WordImportanceColorsSetFit:\n",
    "    \"\"\"\n",
    "    Helper class to quickly colorize sentences based on SetFitGrad.\n",
    "\n",
    "    TODO this class could probably be altered to work with other scores and\n",
    "    other attribution methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scorer: SetFitGrad):\n",
    "        self.scorer = scorer\n",
    "    def show_colors_for_sentence(\n",
    "        self, text: str, integration_steps: int = 100, cmap: str = \"bwr\"\n",
    "    ) -> Tuple[str, pd.DataFrame, float, numpy.array]:\n",
    "        \"\"\"\n",
    "        Pass the output of this function to IPython.display.HTML\n",
    "\n",
    "        from IPython.display import HTML\n",
    "        HTML(colored_text)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        df_w2s, prob, grad_per_integration_step = integrated_gradients_on_text(\n",
    "            text, self.scorer, integration_steps=integration_steps\n",
    "        )\n",
    "\n",
    "        words = df_w2s['words']\n",
    "        scores = df_w2s['score']\n",
    "\n",
    "        a = []\n",
    "        b = []\n",
    "\n",
    "        combined = \"\"\n",
    "        total = 0\n",
    "        for i, word in enumerate(words):\n",
    "            if word[0] == 'Ä ':\n",
    "                a.append(combined)\n",
    "                b.append(total)\n",
    "                combined = word[1:]\n",
    "                total = scores[i]\n",
    "            else:\n",
    "                total += scores[i]\n",
    "                # total =max(total,scores[i])\n",
    "\n",
    "                combined += word\n",
    "        a.append(combined)\n",
    "        b.append(total)\n",
    "\n",
    "        df_w2s = pd.DataFrame({\"words\":a,\"score\":b})\n",
    "        words = a\n",
    "        # words = df_w2s.words.apply(lambda x: x.replace(\"\\u0120\", \"\"))\n",
    "\n",
    "        colors = colorize(df_w2s.score, cmap=cmap)\n",
    "        return (\n",
    "            \" \".join(list(map(hlstr, words, colors))),\n",
    "            df_w2s,\n",
    "            prob.detach(),\n",
    "            grad_per_integration_step,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnToPyTorchLogisticRegression(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pass a trained sklearn LogisticRegression model to this class\n",
    "    to create an equivalent PyTorch model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sklearn_model):\n",
    "        super(SklearnToPyTorchLogisticRegression, self).__init__()\n",
    "\n",
    "        # Extract the parameters from the sklearn model\n",
    "        coef = sklearn_model.coef_\n",
    "        intercept = sklearn_model.intercept_\n",
    "\n",
    "        # Initialize the PyTorch parameters\n",
    "        self.linear = torch.nn.Linear(coef.shape[1], coef.shape[0])\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.copy_(torch.from_numpy(coef))\n",
    "            self.linear.bias.copy_(torch.from_numpy(intercept))\n",
    "        self.linear=self.linear.cuda()\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return torch.softmax(out,dim=-1)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = self.forward(x).round().squeeze().int()\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Compute the predicted probabilities of the positive class for input x\n",
    "        return self.forward(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_html_styles_to_word(html_lines, filename):\n",
    "    # Create a new Word document\n",
    "    doc = Document()\n",
    "    \n",
    "    for html in html_lines:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Create a new paragraph for each line of HTML\n",
    "        p = doc.add_paragraph()\n",
    "        \n",
    "        # Iterate through each part of the parsed HTML\n",
    "        for element in soup.descendants:\n",
    "            if element.name == \"span\" and element.string:\n",
    "                run = p.add_run(element.string)\n",
    "                # Check for style attribute and apply styles if present\n",
    "                style = element.attrs.get('style', '')\n",
    "                color_match = re.search(r'color:\\s*(#[0-9a-fA-F]+)', style)\n",
    "                if color_match:\n",
    "                    color_hex = color_match.group(1)\n",
    "                    run.font.color.rgb = RGBColor(*hex_to_rgb(color_hex))\n",
    "            elif element.name in [\"b\", \"strong\"] and element.string:\n",
    "                run = p.add_run(element.string)\n",
    "                run.bold = True\n",
    "            elif element.name in [\"i\", \"em\"] and element.string:\n",
    "                run = p.add_run(element.string)\n",
    "                run.italic = True\n",
    "            elif element.string:\n",
    "                p.add_run(element.string)\n",
    "    \n",
    "    # Save the document\n",
    "    doc.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diiWh-D6Hkay"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V_UGZYjT9Ex1"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "import re\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= np.load('full_setfit_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_color_(text,score):\n",
    "    colors = colorize(score, cmap=\"bwr\")\n",
    "    words = text.split()\n",
    "    return \" \".join(list(map(hlstr, words, colors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_250={}\n",
    "s=250\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "for i in range(len(test_dataset)):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # print(test_dataset[\"target\"][i],f's={s}')\n",
    "    results_250[i]={'colors':0,'score':0,'model_prediction':preds[i],'new_prediction':0}\n",
    "    try:\n",
    "        colors, d, p = return_prediction(m,test_dataset,preds,i,s)\n",
    "        # print(preprocessing.normalize([d['score'].to_list()]))\n",
    "        results_250[i]['colors']=colors\n",
    "        raw_scores/torch.norm(raw_scores,p=2)\n",
    "        results_250[i]['score']=preprocessing.normalize([d['score'].to_list()])\n",
    "        results_250[i]['new_prediction']=p.argmax().item()\n",
    "\n",
    "    except:\n",
    "        print(i)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_749910/1064817435.py:42: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    results_250[i]['setfit_span']=map_color_(test_dataset[i]['text'],results_250[i]['score'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(results_250,'setfit_250_allelements.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "results_250 =torch.load('setfit_250_raw.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,item in results_250.items():\n",
    "    if item['colors']==0:\n",
    "        print(i)\n",
    "    else:\n",
    "        display(HTML(item['colors']))\n",
    "        # print(item['pred'])\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qF6Cl9dOHeay"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
